{"ast":null,"code":"import { __assign, __awaiter, __extends, __generator } from \"tslib\";\n// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\nimport { Category, Credentials, PredictionsAction, getAmplifyUserAgentObject } from '@aws-amplify/core';\nimport { Storage } from '@aws-amplify/storage';\nimport { AbstractIdentifyPredictionsProvider } from '../types/Providers';\nimport { RekognitionClient, SearchFacesByImageCommand, DetectTextCommand, DetectLabelsCommand, DetectFacesCommand, DetectModerationLabelsCommand, RecognizeCelebritiesCommand } from '@aws-sdk/client-rekognition';\nimport { isStorageSource, isFileSource, isBytesSource, isIdentifyCelebrities, isIdentifyFromCollection } from '../types';\nimport { TextractClient, DetectDocumentTextCommand, AnalyzeDocumentCommand } from '@aws-sdk/client-textract';\nimport { makeCamelCase, makeCamelCaseArray, blobToArrayBuffer } from './Utils';\nimport { categorizeRekognitionBlocks, categorizeTextractBlocks } from './IdentifyTextUtils';\nvar AmazonAIIdentifyPredictionsProvider = /** @class */function (_super) {\n  __extends(AmazonAIIdentifyPredictionsProvider, _super);\n  function AmazonAIIdentifyPredictionsProvider() {\n    return _super.call(this) || this;\n  }\n  AmazonAIIdentifyPredictionsProvider.prototype.getProviderName = function () {\n    return 'AmazonAIIdentifyPredictionsProvider';\n  };\n  /**\n   * Verify user input source and converts it into source object readable by Rekognition and Textract.\n   * Note that Rekognition and Textract use the same source interface, so we need not worry about types.\n   * @param {IdentifySource} source - User input source that directs to the object user wants\n   * to identify (storage, file, or bytes).\n   * @return {Promise<Image>} - Promise resolving to the converted source object.\n   */\n  AmazonAIIdentifyPredictionsProvider.prototype.configureSource = function (source) {\n    return new Promise(function (res, rej) {\n      if (isStorageSource(source)) {\n        var storageConfig = {\n          level: source.level,\n          identityId: source.identityId\n        };\n        Storage.get(source.key, storageConfig).then(function (url) {\n          var parser = /https:\\/\\/([a-zA-Z0-9%\\-_.]+)\\.s3\\.[A-Za-z0-9%\\-._~]+\\/([a-zA-Z0-9%\\-._~/]+)\\?/;\n          var parsedURL = url.match(parser);\n          if (parsedURL.length < 3) rej('Invalid S3 key was given.');\n          res({\n            S3Object: {\n              Bucket: parsedURL[1],\n              Name: decodeURIComponent(parsedURL[2])\n            }\n          });\n        }).catch(function (err) {\n          return rej(err);\n        });\n      } else if (isFileSource(source)) {\n        blobToArrayBuffer(source.file).then(function (buffer) {\n          res({\n            Bytes: new Uint8Array(buffer)\n          });\n        }).catch(function (err) {\n          return rej(err);\n        });\n      } else if (isBytesSource(source)) {\n        var bytes = source.bytes;\n        if (bytes instanceof Blob) {\n          blobToArrayBuffer(bytes).then(function (buffer) {\n            res({\n              Bytes: new Uint8Array(buffer)\n            });\n          }).catch(function (err) {\n            return rej(err);\n          });\n        }\n        if (bytes instanceof ArrayBuffer || bytes instanceof Buffer) {\n          res({\n            Bytes: new Uint8Array(bytes)\n          });\n        }\n        // everything else can be directly passed to Rekognition / Textract.\n        res({\n          Bytes: bytes\n        });\n      } else {\n        rej('Input source is not configured correctly.');\n      }\n    });\n  };\n  /**\n   * Recognize text from real-world images and documents (plain text, forms and tables). Detects text in the input\n   * image and converts it into machine-readable text.\n   * @param {IdentifySource} source - Object containing the source image and feature types to analyze.\n   * @return {Promise<IdentifyTextOutput>} - Promise resolving to object containing identified texts.\n   */\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyText = function (input) {\n    return __awaiter(this, void 0, void 0, function () {\n      var credentials, _a, _b, _c, region, _d, _e, configFormat, inputDocument, err_1, format, featureTypes, textractParam, rekognitionParam, detectTextCommand, rekognitionData, rekognitionResponse, detectDocumentTextCommand, Blocks, err_2, param, analyzeDocumentCommand, Blocks, err_3;\n      return __generator(this, function (_f) {\n        switch (_f.label) {\n          case 0:\n            return [4 /*yield*/, Credentials.get()];\n          case 1:\n            credentials = _f.sent();\n            if (!credentials) return [2 /*return*/, Promise.reject('No credentials')];\n            _a = this._config.identifyText, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.defaults, _e = (_d === void 0 ? {} : _d).format, configFormat = _e === void 0 ? 'PLAIN' : _e;\n            this.rekognitionClient = new RekognitionClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: _getPredictionsIdentifyAmplifyUserAgent()\n            });\n            this.textractClient = new TextractClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: _getPredictionsIdentifyAmplifyUserAgent()\n            });\n            _f.label = 2;\n          case 2:\n            _f.trys.push([2, 4,, 5]);\n            return [4 /*yield*/, this.configureSource(input.text.source)];\n          case 3:\n            inputDocument = _f.sent();\n            return [3 /*break*/, 5];\n          case 4:\n            err_1 = _f.sent();\n            return [2 /*return*/, Promise.reject(err_1)];\n          case 5:\n            format = input.text.format || configFormat;\n            featureTypes = [];\n            if (format === 'FORM' || format === 'ALL') featureTypes.push('FORMS');\n            if (format === 'TABLE' || format === 'ALL') featureTypes.push('TABLES');\n            if (!(featureTypes.length === 0)) return [3 /*break*/, 11];\n            textractParam = {\n              Document: inputDocument\n            };\n            rekognitionParam = {\n              Image: inputDocument\n            };\n            _f.label = 6;\n          case 6:\n            _f.trys.push([6, 9,, 10]);\n            detectTextCommand = new DetectTextCommand(rekognitionParam);\n            return [4 /*yield*/, this.rekognitionClient.send(detectTextCommand)];\n          case 7:\n            rekognitionData = _f.sent();\n            rekognitionResponse = categorizeRekognitionBlocks(rekognitionData.TextDetections);\n            if (rekognitionResponse.text.words.length < 50) {\n              // did not hit the word limit, return the data\n              return [2 /*return*/, rekognitionResponse];\n            }\n            detectDocumentTextCommand = new DetectDocumentTextCommand(textractParam);\n            return [4 /*yield*/, this.textractClient.send(detectDocumentTextCommand)];\n          case 8:\n            Blocks = _f.sent().Blocks;\n            if (rekognitionData.TextDetections.length > Blocks.length) {\n              return [2 /*return*/, rekognitionResponse];\n            }\n            return [2 /*return*/, categorizeTextractBlocks(Blocks)];\n          case 9:\n            err_2 = _f.sent();\n            Promise.reject(err_2);\n            return [3 /*break*/, 10];\n          case 10:\n            return [3 /*break*/, 15];\n          case 11:\n            param = {\n              Document: inputDocument,\n              FeatureTypes: featureTypes\n            };\n            _f.label = 12;\n          case 12:\n            _f.trys.push([12, 14,, 15]);\n            analyzeDocumentCommand = new AnalyzeDocumentCommand(param);\n            return [4 /*yield*/, this.textractClient.send(analyzeDocumentCommand)];\n          case 13:\n            Blocks = _f.sent().Blocks;\n            return [2 /*return*/, categorizeTextractBlocks(Blocks)];\n          case 14:\n            err_3 = _f.sent();\n            return [2 /*return*/, Promise.reject(err_3)];\n          case 15:\n            return [2 /*return*/];\n        }\n      });\n    });\n  };\n  /**\n   * Identify instances of real world entities from an image and if it contains unsafe content.\n   * @param {IdentifyLabelsInput} input - Object containing the source image and entity type to identify.\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to an array of identified entities.\n   */\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyLabels = function (input) {\n    return __awaiter(this, void 0, void 0, function () {\n      var credentials, _a, _b, _c, region, _d, _e, type, inputImage_1, param, servicePromises, entityType, err_4;\n      return __generator(this, function (_f) {\n        switch (_f.label) {\n          case 0:\n            _f.trys.push([0, 3,, 4]);\n            return [4 /*yield*/, Credentials.get()];\n          case 1:\n            credentials = _f.sent();\n            if (!credentials) return [2 /*return*/, Promise.reject('No credentials')];\n            _a = this._config.identifyLabels, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.defaults, _e = (_d === void 0 ? {} : _d).type, type = _e === void 0 ? 'LABELS' : _e;\n            this.rekognitionClient = new RekognitionClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: _getPredictionsIdentifyAmplifyUserAgent()\n            });\n            return [4 /*yield*/, this.configureSource(input.labels.source).then(function (data) {\n              inputImage_1 = data;\n            }).catch(function (err) {\n              return Promise.reject(err);\n            })];\n          case 2:\n            _f.sent();\n            param = {\n              Image: inputImage_1\n            };\n            servicePromises = [];\n            entityType = input.labels.type || type;\n            if (entityType === 'LABELS' || entityType === 'ALL') {\n              servicePromises.push(this.detectLabels(param));\n            }\n            if (entityType === 'UNSAFE' || entityType === 'ALL') {\n              servicePromises.push(this.detectModerationLabels(param));\n            }\n            return [2 /*return*/, Promise.all(servicePromises).then(function (data) {\n              var identifyResult = {};\n              // concatenate resolved promises to a single object\n              data.forEach(function (val) {\n                identifyResult = __assign(__assign({}, identifyResult), val);\n              });\n              return identifyResult;\n            }).catch(function (err) {\n              return Promise.reject(err);\n            })];\n          case 3:\n            err_4 = _f.sent();\n            return [2 /*return*/, Promise.reject(err_4)];\n          case 4:\n            return [2 /*return*/];\n        }\n      });\n    });\n  };\n  /**\n   * Calls Rekognition.detectLabels and organizes the returned data.\n   * @param {DetectLabelsInput} param - parameter to be passed onto Rekognition\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to organized detectLabels response.\n   */\n  AmazonAIIdentifyPredictionsProvider.prototype.detectLabels = function (param) {\n    return __awaiter(this, void 0, void 0, function () {\n      var detectLabelsCommand, data, detectLabelData, err_5;\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            _a.trys.push([0, 2,, 3]);\n            detectLabelsCommand = new DetectLabelsCommand(param);\n            return [4 /*yield*/, this.rekognitionClient.send(detectLabelsCommand)];\n          case 1:\n            data = _a.sent();\n            if (!data.Labels) return [2 /*return*/, {\n              labels: null\n            }]; // no image was detected\n            detectLabelData = data.Labels.map(function (val) {\n              var boxes = val.Instances ? val.Instances.map(function (val) {\n                return makeCamelCase(val.BoundingBox);\n              }) : undefined;\n              return {\n                name: val.Name,\n                boundingBoxes: boxes,\n                metadata: {\n                  confidence: val.Confidence,\n                  parents: makeCamelCaseArray(val.Parents)\n                }\n              };\n            });\n            return [2 /*return*/, {\n              labels: detectLabelData\n            }];\n          case 2:\n            err_5 = _a.sent();\n            return [2 /*return*/, Promise.reject(err_5)];\n          case 3:\n            return [2 /*return*/];\n        }\n      });\n    });\n  };\n  /**\n   * Calls Rekognition.detectModerationLabels and organizes the returned data.\n   * @param {Rekognition.DetectLabelsRequest} param - Parameter to be passed onto Rekognition\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to organized detectModerationLabels response.\n   */\n  AmazonAIIdentifyPredictionsProvider.prototype.detectModerationLabels = function (param) {\n    return __awaiter(this, void 0, void 0, function () {\n      var detectModerationLabelsCommand, data, err_6;\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            _a.trys.push([0, 2,, 3]);\n            detectModerationLabelsCommand = new DetectModerationLabelsCommand(param);\n            return [4 /*yield*/, this.rekognitionClient.send(detectModerationLabelsCommand)];\n          case 1:\n            data = _a.sent();\n            if (data.ModerationLabels.length !== 0) {\n              return [2 /*return*/, {\n                unsafe: 'YES'\n              }];\n            } else {\n              return [2 /*return*/, {\n                unsafe: 'NO'\n              }];\n            }\n            return [3 /*break*/, 3];\n          case 2:\n            err_6 = _a.sent();\n            return [2 /*return*/, Promise.reject(err_6)];\n          case 3:\n            return [2 /*return*/];\n        }\n      });\n    });\n  };\n  /**\n   * Identify faces within an image that is provided as input, and match faces from a collection\n   * or identify celebrities.\n   * @param {IdentifyEntityInput} input - object containing the source image and face match options.\n   * @return {Promise<IdentifyEntityOutput>} Promise resolving to identify results.\n   */\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyEntities = function (input) {\n    return __awaiter(this, void 0, void 0, function () {\n      var credentials, _a, _b, _c, region, _d, celebrityDetectionEnabled, _e, _f, _g, collectionIdConfig, _h, maxFacesConfig, inputImage, param, recognizeCelebritiesCommand, data, faces, err_7, _j, _k, collectionId, _l, maxFaces, updatedParam, searchFacesByImageCommand, data, faces, err_8, detectFacesCommand, data, faces, err_9;\n      var _this = this;\n      return __generator(this, function (_m) {\n        switch (_m.label) {\n          case 0:\n            return [4 /*yield*/, Credentials.get()];\n          case 1:\n            credentials = _m.sent();\n            if (!credentials) return [2 /*return*/, Promise.reject('No credentials')];\n            _a = this._config.identifyEntities, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.celebrityDetectionEnabled, celebrityDetectionEnabled = _d === void 0 ? false : _d, _e = _b.defaults, _f = _e === void 0 ? {} : _e, _g = _f.collectionId, collectionIdConfig = _g === void 0 ? '' : _g, _h = _f.maxEntities, maxFacesConfig = _h === void 0 ? 50 : _h;\n            // default arguments\n            this.rekognitionClient = new RekognitionClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: _getPredictionsIdentifyAmplifyUserAgent()\n            });\n            return [4 /*yield*/, this.configureSource(input.entities.source).then(function (data) {\n              return inputImage = data;\n            }).catch(function (err) {\n              return Promise.reject(err);\n            })];\n          case 2:\n            _m.sent();\n            param = {\n              Attributes: ['ALL'],\n              Image: inputImage\n            };\n            if (!(isIdentifyCelebrities(input.entities) && input.entities.celebrityDetection)) return [3 /*break*/, 7];\n            if (!celebrityDetectionEnabled) {\n              return [2 /*return*/, Promise.reject('Error: You have to enable celebrity detection first')];\n            }\n            _m.label = 3;\n          case 3:\n            _m.trys.push([3, 5,, 6]);\n            recognizeCelebritiesCommand = new RecognizeCelebritiesCommand(param);\n            return [4 /*yield*/, this.rekognitionClient.send(recognizeCelebritiesCommand)];\n          case 4:\n            data = _m.sent();\n            faces = data.CelebrityFaces.map(function (celebrity) {\n              return {\n                boundingBox: makeCamelCase(celebrity.Face.BoundingBox),\n                landmarks: makeCamelCaseArray(celebrity.Face.Landmarks),\n                metadata: __assign(__assign({}, makeCamelCase(celebrity, ['Id', 'Name', 'Urls'])), {\n                  pose: makeCamelCase(celebrity.Face.Pose)\n                })\n              };\n            });\n            return [2 /*return*/, {\n              entities: faces\n            }];\n          case 5:\n            err_7 = _m.sent();\n            return [2 /*return*/, Promise.reject(err_7)];\n          case 6:\n            return [3 /*break*/, 15];\n          case 7:\n            if (!(isIdentifyFromCollection(input.entities) && input.entities.collection)) return [3 /*break*/, 12];\n            _j = input.entities, _k = _j.collectionId, collectionId = _k === void 0 ? collectionIdConfig : _k, _l = _j.maxEntities, maxFaces = _l === void 0 ? maxFacesConfig : _l;\n            updatedParam = __assign(__assign({}, param), {\n              CollectionId: collectionId,\n              MaxFaces: maxFaces\n            });\n            _m.label = 8;\n          case 8:\n            _m.trys.push([8, 10,, 11]);\n            searchFacesByImageCommand = new SearchFacesByImageCommand(updatedParam);\n            return [4 /*yield*/, this.rekognitionClient.send(searchFacesByImageCommand)];\n          case 9:\n            data = _m.sent();\n            faces = data.FaceMatches.map(function (val) {\n              return {\n                boundingBox: makeCamelCase(val.Face.BoundingBox),\n                metadata: {\n                  externalImageId: _this.decodeExternalImageId(val.Face.ExternalImageId),\n                  similarity: val.Similarity\n                }\n              };\n            });\n            return [2 /*return*/, {\n              entities: faces\n            }];\n          case 10:\n            err_8 = _m.sent();\n            return [2 /*return*/, Promise.reject(err_8)];\n          case 11:\n            return [3 /*break*/, 15];\n          case 12:\n            _m.trys.push([12, 14,, 15]);\n            detectFacesCommand = new DetectFacesCommand(param);\n            return [4 /*yield*/, this.rekognitionClient.send(detectFacesCommand)];\n          case 13:\n            data = _m.sent();\n            faces = data.FaceDetails.map(function (detail) {\n              // face attributes keys we want to extract from Rekognition's response\n              var attributeKeys = ['Smile', 'Eyeglasses', 'Sunglasses', 'Gender', 'Beard', 'Mustache', 'EyesOpen', 'MouthOpen'];\n              var faceAttributes = makeCamelCase(detail, attributeKeys);\n              if (detail.Emotions) {\n                faceAttributes['emotions'] = detail.Emotions.map(function (emotion) {\n                  return emotion.Type;\n                });\n              }\n              return {\n                boundingBox: makeCamelCase(detail.BoundingBox),\n                landmarks: makeCamelCaseArray(detail.Landmarks),\n                ageRange: makeCamelCase(detail.AgeRange),\n                attributes: faceAttributes,\n                metadata: {\n                  confidence: detail.Confidence,\n                  pose: makeCamelCase(detail.Pose)\n                }\n              };\n            });\n            return [2 /*return*/, {\n              entities: faces\n            }];\n          case 14:\n            err_9 = _m.sent();\n            return [2 /*return*/, Promise.reject(err_9)];\n          case 15:\n            return [2 /*return*/];\n        }\n      });\n    });\n  };\n  AmazonAIIdentifyPredictionsProvider.prototype.decodeExternalImageId = function (externalImageId) {\n    return ('' + externalImageId).replace(/::/g, '/');\n  };\n  return AmazonAIIdentifyPredictionsProvider;\n}(AbstractIdentifyPredictionsProvider);\nexport { AmazonAIIdentifyPredictionsProvider };\nfunction _getPredictionsIdentifyAmplifyUserAgent() {\n  return getAmplifyUserAgentObject({\n    category: Category.Predictions,\n    action: PredictionsAction.Identify\n  });\n}\n//# sourceMappingURL=AmazonAIIdentifyPredictionsProvider.js.map","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}